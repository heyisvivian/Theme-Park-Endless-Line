{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4a58c3-b759-4001-b0dd-ea14292d6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17347b8c-035d-4b5c-9e44-ae7ed6c1db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_generate_data = False\n",
    "to_train = False\n",
    "to_test = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be07084-64fd-4984-9e3c-e8d69053089b",
   "metadata": {},
   "source": [
    "*Data Preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd974912-f5bd-4e8e-bfd4-72e4fe66a600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40600/40600 [06:50<00:00, 98.89it/s] \n"
     ]
    }
   ],
   "source": [
    "if to_generate_data:\n",
    "    df = pd.read_parquet('autodl-tmp/mins_data.parquet')\n",
    "    df.sort_values(by=['START_TIME', 'Attraction'], inplace=True)\n",
    "    df['lag1'] = df.groupby('Attraction')['WAIT_TIME_MAX'].shift(1).fillna(0)\n",
    "    df['lag2'] = df.groupby('Attraction')['WAIT_TIME_MAX'].shift(2).fillna(0)\n",
    "    df.reset_index(inplace = True,drop = True)\n",
    "    one_hot_columns = [col for col in df.columns if \n",
    "                       pd.api.types.is_numeric_dtype(df[col]) and \n",
    "                       sorted(df[col].unique()) == [0, 1]]\n",
    "    non_one_hot_columns = [col for col in df.columns if \n",
    "                           pd.api.types.is_numeric_dtype(df[col]) and \n",
    "                           col not in one_hot_columns and col not in ['lag1','lag2','WAIT_TIME_MAX']]\n",
    "    scaler = StandardScaler()\n",
    "    df[non_one_hot_columns] = scaler.fit_transform(df[non_one_hot_columns])\n",
    "    \n",
    "    num_attractions = len(df.Attraction.unique())\n",
    "    grouped = df.groupby('START_TIME')\n",
    "    feature_columns = [i for i in list(df) if i not in ['WORK_DATE',\n",
    "     'START_TIME',\n",
    "     'Attraction',\n",
    "     'WAIT_TIME_MAX',\n",
    "     'DEB_TIME',\n",
    "     'FIN_TIME',\n",
    "     'DEB_TIME_x',\n",
    "     'FIN_TIME_x',\n",
    "     'DEB_TIME_y',\n",
    "     'FIN_TIME_y']]\n",
    "    zero_data = np.zeros((1, len(feature_columns)))\n",
    "    zero_df = pd.DataFrame(zero_data, columns=feature_columns)\n",
    "    processed_groups = []\n",
    "    targets = []\n",
    "    all_attractions = sorted(df['Attraction'].unique())\n",
    "    for time_point, group in tqdm(grouped):\n",
    "        processed_group = group.copy()\n",
    "        processed_group = processed_group.drop_duplicates(subset=['Attraction'], keep='first')\n",
    "        present_attractions = group['Attraction'].unique()\n",
    "        missing_attractions = set(all_attractions) - set(present_attractions)\n",
    "        for attraction in missing_attractions:\n",
    "            temp_df = zero_df.copy()\n",
    "            temp_df['Attraction'] = attraction\n",
    "            temp_df['START_TIME'] = time_point\n",
    "            processed_group = pd.concat([processed_group, temp_df], ignore_index=True)\n",
    "        processed_group.sort_values(by=['Attraction'], inplace=True)\n",
    "        t = processed_group.pop('WAIT_TIME_MAX').fillna(0)\n",
    "        data = torch.tensor(processed_group[feature_columns].values, dtype=torch.float)\n",
    "        processed_groups.append(data)\n",
    "        targets.append(t)\n",
    "        if data.shape[0] != 26:\n",
    "            break\n",
    "    features_tensor = torch.stack(processed_groups)\n",
    "    labels_tensor = torch.tensor(np.array(targets))\n",
    "    total_samples = len(features_tensor)\n",
    "    # last 10% for validation\n",
    "    split_idx = int(total_samples * 0.9)\n",
    "    \n",
    "    train_features = features_tensor[:split_idx]\n",
    "    train_labels = labels_tensor[:split_idx]\n",
    "    validation_features = features_tensor[split_idx:]\n",
    "    validation_labels = labels_tensor[split_idx:]\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    validation_dataset = TensorDataset(validation_features, validation_labels)\n",
    "    torch.save(train_dataset, 'autodl-tmp/train_dataset.pth')\n",
    "    torch.save(validation_dataset, 'autodl-tmp/validation_dataset.pth')\n",
    "else:\n",
    "    train_dataset = torch.load('autodl-tmp/train_dataset.pth')\n",
    "    validation_dataset = torch.load('autodl-tmp/validation_dataset.pth')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fbca7-482e-4e6a-a8b4-513d3562d727",
   "metadata": {},
   "source": [
    "*Network design*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d5d20c-cf1f-4eea-a028-f17d0ffce0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network1(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=1):\n",
    "        super(Network1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1500)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1500, 1000)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(1000, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='leaky_relu')\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='leaky_relu')\n",
    "        init.kaiming_normal_(self.fc3.weight, nonlinearity='leaky_relu')\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        intermediate_outputs = {}\n",
    "        out = self.relu(self.fc1(x))\n",
    "        intermediate_outputs['fc1'] = out\n",
    "        out = self.dropout1(out)\n",
    "        intermediate_outputs['dropout1'] = out\n",
    "        out = self.relu(self.fc2(out))\n",
    "        intermediate_outputs['fc2'] = out\n",
    "        out = self.dropout2(out)\n",
    "        intermediate_outputs['dropout2'] = out\n",
    "        out = self.fc3(out)\n",
    "        intermediate_outputs['fc3'] = out\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return intermediate_outputs\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "class Network2(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Network2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc4 = nn.Linear(128, output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        init.kaiming_normal_(self.fc1.weight, nonlinearity='leaky_relu')\n",
    "        init.kaiming_normal_(self.fc2.weight, nonlinearity='leaky_relu')\n",
    "        init.kaiming_normal_(self.fc3.weight, nonlinearity='leaky_relu')\n",
    "        init.kaiming_normal_(self.fc4.weight, nonlinearity='leaky_relu')\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.dropout3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "network1 = Network1(input_size=num_attractions*len(feature_columns),\n",
    "                    output_size=num_attractions*num_attractions).to(device)\n",
    "network2 = Network2(input_size=num_attractions, output_size=num_attractions).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbe3d0e-3162-4159-a92f-3d86cf32b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_matrix(out):\n",
    "    matrix = out.view(26, 26)\n",
    "    normalized_matrix = matrix / matrix.sum(dim=1, keepdim=True)\n",
    "    return normalized_matrix\n",
    "def calculate_stationary_matrix(matrix, num_iterations=30, epsilon=1e-6):\n",
    "    v = torch.rand(matrix.size(0), dtype=matrix.dtype, device=matrix.device)\n",
    "    v = v / v.sum() \n",
    "    for _ in range(num_iterations):\n",
    "        v_next = torch.mv(matrix, v)\n",
    "        v_next = v_next / v_next.sum()  \n",
    "        if torch.norm(v - v_next) < epsilon:\n",
    "            break\n",
    "        v = v_next\n",
    "    return v\n",
    "def Middle(out):\n",
    "    return calculate_stationary_matrix(build_initial_matrix(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa00a87a-57fc-4faf-b0c1-da546a60c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if to_train:\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer1 = optim.Adam(network1.parameters(), lr=0.001)\n",
    "    optimizer2 = optim.Adam(network2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf2dd2-0dd2-4aad-a776-b74b7d976821",
   "metadata": {},
   "source": [
    "*Evaluation Design*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c9ff8ea-c643-4def-b2da-af9b1c6e6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model1, model2, validation_loader, criterion, device):\n",
    "    model1.eval() \n",
    "    model2.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in validation_loader:\n",
    "            inputs = inputs.reshape((1,num_attractions*len(feature_columns))).float()\n",
    "            inputs, targets = inputs.to(device), targets.float().to(device)\n",
    "            out1 = model1(inputs)\n",
    "            intermediate_matrix = build_initial_matrix(out1)\n",
    "            stationary_matrix = calculate_stationary_matrix(intermediate_matrix)\n",
    "            predictions = model2(stationary_matrix)\n",
    "            loss = criterion(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "    return np.sqrt(val_loss / len(validation_loader))\n",
    "def RMSE_for_Attraction(network1,network2):\n",
    "    network1.eval() \n",
    "    network2.eval()\n",
    "    attraction = processed_group.Attraction\n",
    "    result = pd.DataFrame({'attraction':attraction,'RMSE':[0 for i in attraction]})\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in tqdm(validation_loader):\n",
    "            targets = targets.reshape(26)\n",
    "            inputs = inputs.reshape((1,num_attractions*len(feature_columns))).float()\n",
    "            inputs, targets = inputs.to(device), targets.float().to(device)\n",
    "            out1 = network1(inputs)\n",
    "            intermediate_matrix = build_initial_matrix(out1)\n",
    "            stationary_matrix = calculate_stationary_matrix(intermediate_matrix)\n",
    "            predictions = network2(stationary_matrix)\n",
    "            for i in range(len(predictions)):\n",
    "                index = result['attraction'] == attraction[i]\n",
    "                result.loc[index, 'RMSE'] += float((targets[i] - predictions[i]))**2\n",
    "    result.RMSE = result.RMSE/len(validation_loader)\n",
    "    result.RMSE = result.RMSE.apply(lambda x:np.sqrt(x))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63567853-5a18-4a3a-b2f2-acbc86d8100f",
   "metadata": {},
   "source": [
    "*Model Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f388bf-6e8c-4058-b70b-6fe71e173686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 36540/36540 [11:05<00:00, 54.93it/s, Train Loss=362] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 33.4984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 36540/36540 [12:26<00:00, 48.95it/s, Train Loss=1.91e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 21.0608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 36540/36540 [13:51<00:00, 43.94it/s, Train Loss=1.18e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 781.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 36540/36540 [17:36<00:00, 34.57it/s, Train Loss=8.9e+5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 19.1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 36540/36540 [18:29<00:00, 32.94it/s, Train Loss=1.01e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 18.5207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  53%|█████▎    | 19414/36540 [09:44<08:09, 34.96it/s, Train Loss=1.29e+5]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 6: 100%|██████████| 36540/36540 [18:12<00:00, 33.43it/s, Train Loss=1.81e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 21594.4431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 36540/36540 [18:22<00:00, 33.15it/s, Train Loss=2.17e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 20.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 36540/36540 [18:57<00:00, 32.12it/s, Train Loss=8.09e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 15.2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 36540/36540 [18:57<00:00, 32.11it/s, Train Loss=1.52e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 16.5614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   4%|▍         | 1380/36540 [00:43<17:42, 33.09it/s, Train Loss=1.78e+5]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 10: 100%|██████████| 36540/36540 [18:45<00:00, 32.46it/s, Train Loss=2e+4]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 24.9686\n"
     ]
    }
   ],
   "source": [
    "if to_train:\n",
    "    num_epochs = 10\n",
    "    current_best = 1e10\n",
    "    RMSE_val = []\n",
    "    for epoch in range(num_epochs):\n",
    "        network1.train()\n",
    "        network2.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}\")\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs = inputs.reshape((1,num_attractions*len(feature_columns))).float()\n",
    "            inputs, targets = inputs.to(device), targets.float().to(device)\n",
    "            optimizer1.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "            out1 = network1(inputs)\n",
    "            stationary_matrix = Middle(out1)\n",
    "            predictions = network2(stationary_matrix).reshape((1,26)).float()\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            clip_value = 1 #1-10\n",
    "            nn.utils.clip_grad_norm_(network1.parameters(), clip_value) \n",
    "            nn.utils.clip_grad_norm_(network2.parameters(), clip_value)\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'Train Loss': train_loss / (batch_idx + 1)})\n",
    "            \n",
    "        val_loss = validate(network1, network2, validation_loader, criterion, device)\n",
    "        RMSE_val.append(val_loss)\n",
    "        if np.sqrt(val_loss) < current_best:\n",
    "            torch.save(network1, 'network1.pth')\n",
    "            torch.save(network2, 'network2.pth')\n",
    "            current_best = val_loss\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd19c76-d229-4603-9144-d55979230d98",
   "metadata": {},
   "source": [
    "*Best Model Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8532592-870f-44d4-b3ab-7f0166b3a9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for all Attractions 25.199518133114832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4060/4060 [02:15<00:00, 29.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attraction</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumper Cars</td>\n",
       "      <td>5.671665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bungee Jump</td>\n",
       "      <td>14.057108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Circus Train</td>\n",
       "      <td>3.391123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crazy Dance</td>\n",
       "      <td>4.080013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dizzy Dropper</td>\n",
       "      <td>8.861856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Drop Tower</td>\n",
       "      <td>18.153843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flying Coaster</td>\n",
       "      <td>10.705520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Free Fall</td>\n",
       "      <td>24.476853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Giant Wheel</td>\n",
       "      <td>16.283435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Giga Coaster</td>\n",
       "      <td>12.038203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Go-Karts</td>\n",
       "      <td>18.861844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Haunted House</td>\n",
       "      <td>10.644857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Himalaya Ride</td>\n",
       "      <td>3.548672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Inverted Coaster</td>\n",
       "      <td>17.911790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kiddie Coaster</td>\n",
       "      <td>9.637749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Merry Go Round</td>\n",
       "      <td>11.244594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Oz Theatre</td>\n",
       "      <td>4.636091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rapids Ride</td>\n",
       "      <td>13.894598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Roller Coaster</td>\n",
       "      <td>15.448480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Spinning Coaster</td>\n",
       "      <td>11.717141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Spiral Slide</td>\n",
       "      <td>19.813148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Superman Ride</td>\n",
       "      <td>40.455179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Swing Ride</td>\n",
       "      <td>8.562108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Vertical Drop</td>\n",
       "      <td>17.795413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Water Ride</td>\n",
       "      <td>12.200764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Zipline</td>\n",
       "      <td>8.841346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          attraction       RMSE\n",
       "0        Bumper Cars   5.671665\n",
       "1        Bungee Jump  14.057108\n",
       "2       Circus Train   3.391123\n",
       "3        Crazy Dance   4.080013\n",
       "4      Dizzy Dropper   8.861856\n",
       "5         Drop Tower  18.153843\n",
       "6     Flying Coaster  10.705520\n",
       "7          Free Fall  24.476853\n",
       "8        Giant Wheel  16.283435\n",
       "9       Giga Coaster  12.038203\n",
       "10          Go-Karts  18.861844\n",
       "11     Haunted House  10.644857\n",
       "12     Himalaya Ride   3.548672\n",
       "13  Inverted Coaster  17.911790\n",
       "14    Kiddie Coaster   9.637749\n",
       "15    Merry Go Round  11.244594\n",
       "16        Oz Theatre   4.636091\n",
       "17       Rapids Ride  13.894598\n",
       "18    Roller Coaster  15.448480\n",
       "19  Spinning Coaster  11.717141\n",
       "25      Spiral Slide  19.813148\n",
       "20     Superman Ride  40.455179\n",
       "21        Swing Ride   8.562108\n",
       "22     Vertical Drop  17.795413\n",
       "24        Water Ride  12.200764\n",
       "23           Zipline   8.841346"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if to_test:\n",
    "    best_network1 = torch.load('autodl-tmp/network1.pth')\n",
    "    best_network2 = torch.load('autodl-tmp/network2.pth')\n",
    "    val_loss = validate(network1, network2, validation_loader, criterion, device)\n",
    "    print('RMSE for all Attractions',val_loss)\n",
    "    RMSE_for_Attraction(best_network1,best_network2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
